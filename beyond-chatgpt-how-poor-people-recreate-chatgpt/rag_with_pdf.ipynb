{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/martin-ku-hku/ai-sharing/blob/main/beyond-chatgpt-how-poor-people-recreate-chatgpt/falcon_rag_with_pdf.ipynb)\n",
        "\n",
        "Make sure that you use GPU in the Colab environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mWV9IK_uZU0"
      },
      "source": [
        "# RAG with PDF\n",
        "\n",
        "Experiment: To test loading PDF documents to a local Chroma vector DB, and performing RAG with the vector DB and a LLM (with 4 bit quantization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xp4B25suZU6"
      },
      "source": [
        "## Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtdYlSH5uZU7"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers sentence-transformers datasets accelerate einops langchain xformers bitsandbytes gradio pypdf tiktoken chromadb lark gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ9byPsVuZU9"
      },
      "source": [
        "## Loading a PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9EO4mkRFrjn"
      },
      "outputs": [],
      "source": [
        "!wget -O testing.pdf https://d2tic4wvo1iusb.cloudfront.net/production/documents/guidance/Cognitive_science_approaches_in_the_classroom_-_A_review_of_the_evidence.pdf?v=1691716109"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yku5qVaJuZU9"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqm2jM22uZU-"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader('testing.pdf')\n",
        "pages = loader.load()\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo4nlR2RuZU_"
      },
      "outputs": [],
      "source": [
        "page = pages[10]\n",
        "print(page.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2-TLaGjuZVA"
      },
      "source": [
        "## Splitting document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EbZX2kguZVA"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euatXVCjuZVB"
      },
      "outputs": [],
      "source": [
        "chunk_size = 1500\n",
        "chunk_overlap = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfjnSxnPuZVB"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpldw_r2uZVC"
      },
      "outputs": [],
      "source": [
        "texts = text_splitter.split_documents(pages)\n",
        "print(len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hovfh-w3uZVC"
      },
      "outputs": [],
      "source": [
        "for text in texts:\n",
        "    print(text.page_content)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiJ4FmVUuZVC"
      },
      "source": [
        "## Initialize the HuggingFace Embedding Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-vEdcUAuZVC"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhNIEqTfuZVD"
      },
      "outputs": [],
      "source": [
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GviXlwvCuZVD"
      },
      "source": [
        "## Vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9e-gAFouZVD"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b7zddNbuZVD"
      },
      "outputs": [],
      "source": [
        "persist_directory = '../vdb/chroma'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C32kC-7uZVD"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embed_model,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "datF4tRZuZVE"
      },
      "outputs": [],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn5rKM5ICx6Y"
      },
      "outputs": [],
      "source": [
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTBcRZ5Izbwb"
      },
      "source": [
        "## Similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwEV-sHgzdpW"
      },
      "outputs": [],
      "source": [
        "question = \"What is spaced practice in the classroom?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVwIk5o136Pe"
      },
      "outputs": [],
      "source": [
        "retrieved = vectordb.similarity_search(question, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXeTOFcO4Ckw"
      },
      "outputs": [],
      "source": [
        "for doc in retrieved:\n",
        "  print(doc.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8Vk798b4JHX"
      },
      "outputs": [],
      "source": [
        "for doc in retrieved:\n",
        "  print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MjMlWJ9L-6O"
      },
      "source": [
        "## MMR search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7EGPyK65nNT"
      },
      "outputs": [],
      "source": [
        "mmr_retrieved = vectordb.max_marginal_relevance_search(question, k=3, fetch_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyfxA7GJ507Q"
      },
      "outputs": [],
      "source": [
        "for doc in mmr_retrieved:\n",
        "  print(doc.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMAk9AVi55uX"
      },
      "outputs": [],
      "source": [
        "for doc in mmr_retrieved:\n",
        "  print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A70x5GhHvyud"
      },
      "source": [
        "## HuggingFace Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjAN7rhHv59F"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'bigscience/bloom-7b1'\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtInONRwAuX"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-SrzbcwFi5"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxdiDjYqygw3"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True, # langchain expects full text,\n",
        "    task='text-generation',\n",
        "    temperature=0.0,\n",
        "    max_new_tokens=128,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPpymc-ZykZb"
      },
      "source": [
        "## Load the pipeline in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUg52azhyod8"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPM_o_LTuZVE"
      },
      "source": [
        "## Question answering with the document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YZRnQOvOGfz"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyTi8dvLONhQ"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(search_type='mmr')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUMjYoeZuZVE"
      },
      "outputs": [],
      "source": [
        "question = \"What are the practices that teachers might use to manage cognitive load?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NksU70zeOf6i"
      },
      "outputs": [],
      "source": [
        "result = qa_chain({'query': question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUHeHQjoOk_C"
      },
      "outputs": [],
      "source": [
        "result['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTES75WxOqSi"
      },
      "outputs": [],
      "source": [
        "# no RAG, use the LLM only\n",
        "llm(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpLbmvUSULx"
      },
      "source": [
        "## With extra prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1exfAZNOyT1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw88ia_LS-7Q"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say you don't know, don't try to make up an answer. Keep the answer as concise as possible. Always say \"thanks for asking\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRfzrrBPTr-4"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgZGnY10WpyA"
      },
      "outputs": [],
      "source": [
        "question = \"What are the practices that teachers might use to manage cognitive load?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xKvw_SXWu1H"
      },
      "outputs": [],
      "source": [
        "result = qa_chain({'query': question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRwbrGGxXGIL"
      },
      "outputs": [],
      "source": [
        "result['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjtVzHJ_XGj4"
      },
      "outputs": [],
      "source": [
        "# no RAG, use LLM only\n",
        "llm(question)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
